<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Hoang NT


  | Finding Lane Lines on the Road

</title>
<meta name="description" content="Hoang NT's personal website.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="/assets/css/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚙️</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2017/basic-lane-detection/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N8K9D9WKRW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-N8K9D9WKRW');
</script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Finding Lane Lines on the Road",
      "description": "Getting started with OpenCV",
      "published": "February 26, 2017",
      "authors": [
        
        {
          "author": "Hoang NT",
          "authorURL": "/",
          "affiliations": [
            {
              "name": "Titech",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://gearons.org/">
       Hoang NT
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Finding Lane Lines on the Road</h1>
        <p>Getting started with OpenCV</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <p><a href="https://github.com/gear/CarND/tree/master/lanelines-p1">p1-lanelines on Github</a></p>

<h2 id="opencv-toolbox">OpenCV Toolbox</h2>

<p>OpenCV is an image processing toolbox originally developed in C++.
In Python, an OpenCV image is a <code class="language-plaintext highlighter-rouge">numpy</code> array (2D or 3D depending
on the type of image). The figure below depicts the coordination
used in OpenCV. For example, if we have a numpy array <code class="language-plaintext highlighter-rouge">img</code> describing
an OpenCV image, then <code class="language-plaintext highlighter-rouge">img[0,0]</code> stores the data for top left
pixel having the coordinate <code class="language-plaintext highlighter-rouge">(x=0,y=0)</code>. Following this system, the
bottom right corner <code class="language-plaintext highlighter-rouge">img[-1,-1]</code> has the coordinate
<code class="language-plaintext highlighter-rouge">(x=img.shape[1]-1, y=img.shape[0]-1)</code> in OpenCV.</p>

<div class="row mt-3">
     <div class="col-sm mt-3 mt-md-0">
         <img class="img-fluid rounded z-depth-0 centering-img" src="/assets/img/image_coo.png" width="50%" data-zoomable="">
     </div>
 </div>
<div class="caption">
OpenCV Image
</div>

<p>OpenCV provides various tools for us to “get our hand dirty”
with images. Generally, there are two main groups: image drawing and
image transformation. 
<a href="http://docs.opencv.org/3.0-beta/modules/imgproc/doc/drawing_functions.html">Drawing on images in OpenCV</a> is quite simple
and straight forward (except for the ellipse <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">) as most drawing
function is in the form:</p>

<d-code block="" language="python">
 import cv2
 cv2.(img_to_draw_on, starting_point, **others_arguments)
</d-code>

<h2 id="line-detection-pipeline">Line Detection Pipeline</h2>

<p>The main tool to detect lines in an image is a technique named
<em>Hough Transformation</em>. It is called <em>transformation</em> because
it transforms the representation of a line to a pair of line angle
and line distance to the origin. The detail and tutorial of
Hough Transformation is provided
<a href="http://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.html">here</a>. To reduce the computation and increase
the accuracy of the line detection pipeline, we focus only on the
area in front of a car. The following list describes our pipeline:</p>

<ol>
  <li>
<strong>Extract the region of interest (ROI)</strong>. In this case, the ROI is a trapezoid in front of the car.</li>
  <li>
<strong>Create a color mask</strong>. Lucky for us, the lane lines of interest only have white or yellow colors. Therefore, extracting only yellow and white color will greatly reduce the computation for unwanted objects in the image.</li>
  <li>
<strong>Canny edge detection</strong>. Performing the Canny edge detection algorithm on the color filtered image greatly reduces number of points needed to process in the next step.</li>
  <li>
<strong>(Probabilistic) Hough line transform</strong>. Given a set of points from edge detection, we detect lines using Hough transformation.</li>
  <li>
<strong>Split lines into left and right set</strong>. The output of Hough line transformation is a set of lines, represented by two points <code class="language-plaintext highlighter-rouge">(x1,y1)</code> and <code class="language-plaintext highlighter-rouge">(x2,y2)</code>. For each pair of points in the returned set, we split them into left lane points and right lane points by its coefficient.</li>
  <li>
<strong>Fitting lines to left and right points</strong>. We get two lines for left lane and right lane by fitting a line to each set of points. The output of OpenCV’s line fitting algorithm is a 4-tuple: <code class="language-plaintext highlighter-rouge">(x0, y0, vx, vy)</code>, where <code class="language-plaintext highlighter-rouge">(x0,y0)</code> is a point on the line and <code class="language-plaintext highlighter-rouge">(vx,xy)</code> is the line’s co-linear vector.</li>
  <li>
<strong>Drawing lines onto the image</strong>. To draw a line onto an image, we need two points (start and end). We can compute these points for drawing from the output of step 6 and a specified drawing zone (the trapezoid ROI for example).</li>
</ol>

<hr>

<h2 id="reflection">Reflection</h2>

<h3 id="color-space-and-region-of-interest">Color space and region of interest</h3>

<p>The first pipeline we came up with in this project simply converts the
input to gray scale, detect edges by Canny algorithm, and then draw all
line that Hough transformation returns. This approach is unstable since
it depends heavily on the high and low threshold of the Canny algorithm.
Although using a slight Gaussian blur on the gray scale image can
reduce noise and improve the quality of the pipeline, imperfection on
the road can potentially disturb the pipeline’s robustness.</p>

<p>The unused information in our first pipeline is: 1. The region of
interest, and 2. The color of the lane lines. By extracting the region
of interest, we eliminate unnecessary computation:</p>

<p><img src="/assets/img/focus_region.png" alt="Focus region" width="70%" class="center-small"></p>

<p>In addition to extracting the region of interest, we also filtered out
unwanted colors. By default, the image output of <code class="language-plaintext highlighter-rouge">cv2.imread</code> is a GBR
image. This color representation makes it hard to filter a certain
color since all three values (G,B,R) of a pixel represents color.
Therefore, we convert the image to HSV color space
(Hue, Saturation, Value). In HSV images, a pixel contains the color
(hue), the “amount” of that color (saturation), and its brightness
(value). This color representation enable us to specify the colors we
want to extract. To exact colors, the rule of thumb is to range
±10 in the hue value as following:</p>

<d-code block="" language="python">
hue_range = 10  # Increase for wider color selection
# rgb_color is the (R,G,B) tuple value of the color we want to filter
pixel = np.uint8([[rgb_color]])  # One pixel image
hsv_pixel = cv2.cvtColor(pixel, cv2.COLOR_RGB2HSV)  # Convert to HSV
hue = hsv_pixel[0,0,0]  # Get the hue value of the input (R,G,B)
lowb = np.array((hue-hue_range, 100, 100), dtype=np.uint8)
upb = np.array((hue+hue_range, 255, 255), dtype=np.uint8)
return lowb, upb  # Lower and upper bound for color filtering
</d-code>

<p>To exact black or white color, the code is different since it depends
on the saturation and value rather than the hue.</p>

<d-code block="" language="python">
sensitivity = 30
lowwhite = np.array((0,0,255-sensitivity), dtype=np.uint8)
upwhite = np.array((255,sensitivity,255), dtype=np.uint8)
return lowwhite, upwhite  # Lower and upper bound for color filtering
</d-code>

<p>After selecting only the region of interest and the colors, we have
the following result:</p>

<p><img src="/assets/img/filtered_roi.png" alt="Focus region" width="70%" class="center-small"></p>

<p>The image above is a binary image which can be used as a mask to
extract the lane lines from the original image. The example of our
lane lines detection on static image is shown below.</p>

<p><img src="/assets/img/result_lanelines.png" alt="Result on image" width="70%" class="center-small"></p>

<h3 id="buffered-pipeline">Buffered pipeline</h3>

<p>The pipeline showed in the previous session performs well on test
images and videos. However, with the challenge video, it failed to
detect the lane lines for some brief moments when the lighting varies.
Furthermore, in all videos, the lane lines between frame doesn’t have
smooth transitions. To address this problem, we have several approaches:</p>

<ol>
  <li>Limit the movement of lines between frames. We specify a limit
$\alpha$ for the displacement of two lines between adjacent frames. The
next frame’s line is computed as: \(x_t = x_{t-1} + \min{(\alpha, x_t -
x_{t-1})}\)</li>
  <li>Store previous lines in a fixed-size buffer, add new line to the
buffer for every frame. The output is the weighted average of all the
lines in the buffer.</li>
  <li>Similar to the second approach, but instead of storing line points,
we store the lines’ co-linear vectors. The next line’s co-linear vector
is the weighted average of the vectors stored in the buffer.</li>
</ol>

<p>The videos result for each of the approach will be updated soon. TODO:
Upload videos.</p>

<p>We have some minor bugs during the implementation of of the buffered
pipeline. Firstly, when the buffer is empty, the pipeline should not
draw the line. In one of our implementation, a default line is drawn
when the buffer is empty, this design decision makes it hard to debug
the program. Secondly, when no line is detected from the frame, the
algorithm should still return a line from the buffer. However, if there
are many “no line” frames, there is a chance that there isn’t any lane
lines on the road. Our current buffer implementation hasn’t taken care
of this situation.</p>

<h3 id="unnecessary-operations">Unnecessary operations</h3>

<p>For the current testing data (images and videos), extracting the region
of interest and lane line colors is enough for line detection.</p>

<p><img src="/assets/img/only_color.png" alt="Result on image" class="center-small"></p>

<p>As the picture above has shown, only yellow (left) and white (right)
color filter is enough to give us a substantially clear image of lane
lines. This output here can be put directly to the Hough Line detection
(without masking with the original image or Canny edge detection) to
obtain the lane lines. At this stage, we don’t know if performing Canny
edge detection is necessary (i.e. makes the pipeline more robust)  <img class="emoji" title=":confused:" alt=":confused:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f615.png" height="20" width="20">.</p>

<p>Thanks for reading! <img class="emoji" title=":sunny:" alt=":sunny:" src="https://github.githubassets.com/images/icons/emoji/unicode/2600.png" height="20" width="20"></p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    © Copyright 2022 Hoang  NT.
    
    
    
    Last updated: October 11, 2022.
    
  </div>
</footer>



  </body>

  <d-bibliography src="/assets/bibliography/">
  </d-bibliography>

</html>
