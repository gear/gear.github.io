<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Gearons | Tensorizing Recurrent Neural Nets</title>
  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="Tensorizing Recurrent Neural Nets">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://gearons.org//posts/tlstm">
  <meta property="og:description" content="">
  <meta property="og:site_name" content="Gearons">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="https://gearons.org//posts/tlstm">
  <meta name="twitter:title" content="Tensorizing Recurrent Neural Nets">
  <meta name="twitter:description" content="">

  
    <meta property="og:image" content="https://gearons.org//assets/og-image-ee46bbc61b334e821e81534b1fd43f3fee6f020ec174b3c2114445695fd48c01.jpg">
    <meta name="twitter:image" content="https://gearons.org//assets/og-image-ee46bbc61b334e821e81534b1fd43f3fee6f020ec174b3c2114445695fd48c01.jpg">
  

  <link href="https://gearons.org//feed.xml" type="application/rss+xml" rel="alternate" title="Gearons Last 10 blog posts" />

  

  

    
      <link rel="icon" type="image/x-icon" href="/assets/favicon-light-b2624f1aef1507a57b8ae1e334ba18341523adcff511393365e33e6c4cdc007b.ico">
      <link rel="apple-touch-icon" href="/assets/apple-touch-icon-light-87d1f2a3a19b1500e5c1626a0492025ca5f7f97d24540dc5900288e92112925a.png">
      <link rel="stylesheet" type="text/css" href="/assets/light-d5cd0e3eaa66b2ed98fb88a8443522c2a074034a960d3e41d1ca589152717bac.css">
    

  

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script> 
</head>

<body>
  <main>
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav scrollappear">
  <a href="/" class="header-logo" title="Gearons">Gearons</a>
  <ul class="header-links">
    
      <li>
        <a href="/about" title="About me">
          <i style="vertical-align: sub; color: #018A7B;" class="material-icons">settings_applications</i>


        </a>
      </li>
    
    
    
      <li>
        <a href="https://scholar.google.com/citations?user=iuSBSHsAAAAJ&amp;hl=en" rel="noreferrer noopener" target="_blank" title="Google Scholar">
          <i style="vertical-align: sub; color: #018A7B;" class="material-icons">school</i>


        </a>
      </li>
    
    
    
      <li>
        <a href="https://github.com/gear" rel="noreferrer noopener" target="_blank" title="GitHub">
          <i style="vertical-align: sub; color: #018A7B;" class="material-icons">code</i>


        </a>
      </li>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
      <li>
        <a href="https://steamcommunity.com/id/gearons" rel="noreferrer noopener" target="_blank" title="Steam">
          <i style="vertical-align: sub; color: #018A7B;" class="material-icons">videogame_asset</i>


        </a>
      </li>
    
    
    
    
    
    
    
  </ul>
</nav>



        <article class="article scrollappear">
          <header class="article-header">
            <h1>Tensorizing Recurrent Neural Nets</h1>
            <p></p>
            <div class="article-list-footer">
  <span class="article-list-date">
    February 2, 2018
  </span>
  <span class="article-list-divider">-</span>
  <span class="article-list-minutes">
    
    
      8 minute read
    
  </span>
  <span class="article-list-divider">-</span>
  <div class="article-list-tags">
    
  </div>
</div>
          </header>

          <div class="article-content">
            <p>This post is the summarization of the paper “Wider and Deeper, Cheaper and Faster:
Tensorized LSTMs for Sequence Learning” by Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu
Hangen He, and David Barber. Published as a conference paper at NIPS 2017, Long Beach, CA, USA.</p>

<p><a href="https://papers.nips.cc/paper/6606-wider-and-deeper-cheaper-and-faster-tensorized-lstms-for-sequence-learning.pdf">Paper</a>, <a href="https://papers.nips.cc/paper/6606-wider-and-deeper-cheaper-and-faster-tensorized-lstms-for-sequence-learning-supplemental.zip">supplementary materials</a></p>

<hr>

<h2 id="main-ideas-and-contributions">Main ideas and contributions</h2>

<ol>
  <li>
    <p>Starting from a skewed stacked RNN architecture, the authors proposed <strong>a novel
RNN where each hidden unit is parameterized by a high rank tensor (≥2)</strong>.</p>

    <p><img src="/img/trnn.png" alt="sRNN and tRNN"></p>

    <p>The image above compares a three hidden layers (depth L = 3) skewed recurrent neural network with the proposed model.
 The tRNN model here has only one hidden layer, but its hidden units are parameterized by a P-by-M matrix rather than
 a single vector. Furthermore, the interaction between hidden units are captured by the convolution operation (defined
 later) to take advantages of the tensor representation. According to the authors, the first dimension of the hidden units is “locally connected” in order to share parameters, while the second dimension is “fully connected” for global interaction. This idea will be clearer when we discuss the convolution. In general, the construction of a hidden unit output is computed as following (<script type="math/tex">\circledast</script> is the convolution operator):</p>

    <script type="math/tex; mode=display">H^\text{cat}_{t-1, p} = \left\{ 
 \begin{array}{l}
 x_t W^x + b^x, \text{ if } p = 1 \\ 
 h_{t-1, p-1}, \text{ else } %_
 \end{array}
 \right. \ \ \ \
 \begin{array}{l}
 A_t = H^\text{cat}_{t-1} \circledast \{W^h, b^h\} \\
 H_t = \phi(A_t) \\
 y_t = \varphi(h_{t+L-1, p} W^y + b^y)
 \end{array}</script>

    <p>Note that the actual output at <script type="math/tex">y_t</script> is computed using the last tensor (in this case it is a vector) of 
 the hidden tensor <script type="math/tex">H_{t+L-1}</script>. As in the figure above, <script type="math/tex">y_t</script> is computed from <script type="math/tex">h^3_{t+2}</script> - the last 
 layer of the hidden unit <script type="math/tex">H_{t+2}</script>. Also, the implicit depth P = L = 3 leads us to the same computation.</p>
  </li>
  <li>
    <p>The paper introduces <strong>cross-layer convolution and memory cell convolution (for the LSTM extension)</strong>. The explanation
to the convolution is presented in the second section. Note that in this post, I moved the time notation to the top of
each symbol when it’s convenient so that the subscript contains only indexing variables (e.g. <script type="math/tex">A_{t, p, m^o}</script> becomes <script type="math/tex">A^t_{p, m^o}</script>).</p>
  </li>
  <li>
    <p>The 2D-tRNN model is further <strong>extended to LSTM and higher order tensors (3D)</strong>.</p>

    <script type="math/tex; mode=display">\begin{array}{l}
 [A^g_t, A^i_t, A^f_t, A^o_t] = H^\text{cat}_{t-1} \circledast \{W^h, b^h\} \\
 [G_t, I_t, F_t, O_t] = [\phi{A^g_t}, \sigma{A^i_t}, \sigma{A^f_t}, \sigma{A^o_t}] \\
 \text{Memory cell: } C_t = G_t \odot I_t + C_{t-1} \odot F_t \in R^{P \times M} \\
 H_t = \phi(C_t) \odot O_t 
 \end{array}</script>

    <p>This extension to LSTM is pretty straight-forward as each gate is computed using the convolution operator
 <script type="math/tex">\circledast</script> in lieu of the standard matrix multiplication. For the higher order tensors extension, the concatenated
 tensor is constructed by appending the projection (multiplied with weights, added with bias) of <script type="math/tex">x_t</script> to one 
 <em>corner</em> of the hidden unit tensor. This can be understood as going down the tensor dimension-wise, when you reach
 a 2D matrix with row size of M, append <script type="math/tex">x_t W^x + b^x</script> to it. In the same way, <script type="math/tex">y_t</script> is generated with the
 opposite corner.</p>
  </li>
</ol>

<h2 id="dissecting-convolution">Dissecting convolution</h2>

<p>It took me sometime to fully grasp the author’s idea <img class="emoji" title=":dizzy_face:" alt=":dizzy_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f635.png" height="20" width="20">. Despite that “sometime”, my understanding can be
captured in a single image (and now I feel silly). In the case of 2D hidden units, the operation and the dimensionality 
of each tensor is:</p>

<script type="math/tex; mode=display">A_t = H^\text{cat}_{t-1} \circledast \{W^h, b^h\}, \text{ where: } \ \ \  
\begin{array}{l}
A_t \in R^{P \times M^o} \\
H^\text{cat}_{t-1} \in R^{(P+1) \times M} \\
W^h \in R^{K \times M^i \times M^o} \\
b^h \in R^{M^o}
\end{array}</script>

<p>In here, <script type="math/tex">A_t</script> is the activation tensor. <script type="math/tex">H^\text{cat}_{t-1}%_</script> represents the concatenation of a hidden unit’s 
rank-2 tensor output (matrix) and the input vector from the layer above it (in this case, it is the input vector 
<script type="math/tex">x_t</script>). From the skewed sRNN figure above, the concatenation is pretty clear: <script type="math/tex">h_t</script> is pointed to by <script type="math/tex">x_t</script> and
<script type="math/tex">h_{t-1}</script>, hence the tensor used in computing <script type="math/tex">A_t</script> and <script type="math/tex">h_t</script> will be the concatenation between <script type="math/tex">x_t</script> and
<script type="math/tex">h_{t-1}</script>. The same principle applies for the tRNN. The convolution kernel consists of weight <script type="math/tex">W^h</script> and bias <script type="math/tex">b^h</script>.
<script type="math/tex">W^h</script> is a rank-3 tensor of K filters, each filters has <script type="math/tex">M^i</script> input channels and <script type="math/tex">M^o</script> output channels. In this
paper, the authors noted that they let <script type="math/tex">M = M^i = M^o</script> for simplicity. The detail of the convolution is given in
the supplementary document:</p>

<script type="math/tex; mode=display">A^t_{p, m^o} = \sum^K_{k=1} \left( \sum^{M^i}_{m^i=1} H^{cat; t-1}_{p - \frac{K-1}{2}+k, m^i} \cdot W^k_{k,m^i,m^o}  \right) + b^h_{m^o}</script>

<p>It might sound obvious, but it is easier for me to think of the indices as “selector”. For example, <script type="math/tex">A^t_{p, m^o}</script>
is the element <script type="math/tex">\square</script> of tensor <script type="math/tex">A^t</script> that is selected by <script type="math/tex">(p, m^o)</script>. Usually, the “selector” is lowercase
and the total number of element in a dimension is uppercase. In the figure below, I circled in red the “selectors”.</p>

<p><img src="/img/rnn_conv.png" alt="sRNN and tRNN"></p>

<p>It might helps with a kind of story. The matrix <script type="math/tex">A^t \in R^{P \times M^o}</script> is what we need to compute for the next time 
step. We set out to compute each element of this matrix. <script type="math/tex">A^t_{p, m^o}</script> is given by the convolution operator defined
in the formula above. In the figure, the gray small box represents <script type="math/tex">A^t_{p, m^o}</script>. The two selector <script type="math/tex">p</script> and
<script type="math/tex">m^o</script> control the center of the convolution in <script type="math/tex">H^{\text{cat}; t-1}</script> and which output channel to pick in <script type="math/tex">W^h</script> 
respectively. For example, if <script type="math/tex">K=5</script> and where want to computer <script type="math/tex">A^t</script> at <script type="math/tex">(p=3, m^o=3)</script>, then for each slice of
<script type="math/tex">W^h</script>, the 5 columns associated with <script type="math/tex">m^o=3</script> will be picked out. Next, on <script type="math/tex">H^{\text{cat}; t-1}</script>, there are also 5 
rows selected. Since <script type="math/tex">p=3</script>, the index of these row vectors are <script type="math/tex">\{2,3,4,5,6\}</script> (centered at <script type="math/tex">p+1=4</script>). These 5 pairs
of vectors are indexed by <script type="math/tex">k = \{1,2,3,4,5\}</script>. Sum of the dot products of the pairs is <script type="math/tex">A^t_{p, m^o}</script>.
Note that the authors use zero-padding to keep the shape of output same as the input. In the case of memory cell 
convolution, the values used in padding are the border values to avoid interference with the memory values.</p>

<h2 id="evaluations">Evaluations</h2>

<p>The authors uses three main tasks to demonstrate the effectiveness of their proposed model:</p>

<ol>
  <li>
    <p><strong>Algorithmic tasks</strong> consist of learning to sum two 15-digit numbers and learning to repeat the input sequence.</p>

    <div class="highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>  Sum two 15-digit integers    ||   Repeat the input
  Input : --2545-9542-------   ||   Input : -hiworld----------
  Output: -----------12087--   ||   Output: --------hiworld--- 
</code></pre></div>    </div>
    <p>The advantage of the proposed model (3D-tLSTM with Channel Normalization) is that it requires much less training
 samples to reach more than 99 percent accuracy compared with Stacked LSTM and Grid LSTM. However, there are no
 mention to the training time or other modification of tLSTM. Furthermore, the best performing models have 7 and 10
 layers depth. There are not much explanation for these hyper-parameters. The 7-layer requires less training samples
 than the 10-layer in addition task, but the 10-layer in turn requires much less samples in memorization. I expected
 the 7-layer model requires less samples in both tasks.</p>
  </li>
  <li>
    <p><strong>MNIST classification</strong> tasks consist of normal MNIST and randomized pixel sequence called pMNIST. In these tasks, 
the pixels of an hand-written digit image are fed into the neural nets as a sequence. In this task, I do not see any
advantage of using tLSMT compared with state-of-the-art methods such as DilatedGRU.</p>
  </li>
  <li>
    <p><strong>Wikipedia language modeling</strong> task deal with next-character prediction on the Hutter Prize Wikipedia dataset. Similar
to the aforementioned MNIST tasks, there are no clear win for tLSTM compared with Large FS-LSTM-4 method (in term of
both BPC and number of parameters).</p>
  </li>
</ol>

<h2 id="why-tensors">Why tensors?</h2>

<p>It is true that we can design a “vector” recurrent neural net that works in the same way as the proposed tRNN here. After
all, the variables is stored in arrays on our computer. However, tensors represent a concrete way to think about a “block
of numbers”, and more importantly, we can define and argue about convolution easily with it. Let’s take image data as 
an example. The image below shows the popular 2D convolution operation. Tensor representation gives us a concrete way
to think about color channels and its corresponding channels in each convolution kernel. Furthermore, notice that we
“scan” each convolution channel in the intensity dimension (on the matrices) but not the color dimension. As such, the
weights in the intensity dimension are shared. Similarly, in recurrent neural networks, the tensor hidden unit in some
sense enables a more abstract representation and allow weight sharing.</p>

<p><img src="/img/2d_conv.png" alt="sRNN and tRNN"></p>

<h2 id="conclusion">Conclusion</h2>

<p>This paper proposes a nice way to increase the structural depth of a recurrent neural network model. Instead of 
parameterizing hidden units with vectors, tRNN modeled hidden units as tensors for more efficient weight sharing
and implicit depth. Such approach greatly reduces the number of parameters in a recurrent neural network while 
maintaining the depth. tRNN model is also extended to use LSTM module and higher order (3D) tensor for better performance
and data abstraction. The effectiveness of tLSTM is demonstrated on three different tasks in which tLSTM achieve 
state-of-the-art performance with some improvement on number of parameters (minor) and required number of training
samples. On the other hand, while the proposed model might improve running time and number of parameters, there was no
discussion on the training time and training complexity of tLSTM. It would be interesting to implement the 2D and 3D models
to understand the benefit of tLSTM better.</p>

<p>In this post, I wrote about my understanding and commented on the paper “Wider and Deeper, Cheaper and Faster:
Tensorized LSTMs for Sequence Learning” by Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, and David Barber.
I left out some details such as Channel Normalization or the constraints on <script type="math/tex">\{L, P, K\}</script>. These minor optimization
tricks can be found on the paper. After this post, I plan to implement this technique to see if it is suitable for my
current work. In particular, I would like to see if the training cost can potentially be reduced for a stacked LSTM
approach to model product names.</p>

          </div>
          <div class="article-share">
            
            
            <a href="https://twitter.com/home?status=Tensorizing+Recurrent+Neural+Nets%20-%20https://gearons.org//posts/tlstm" title="Share on Twitter" rel="noreferrer noopener" target="_blank">
              <svg viewbox="0 0 512 512"><path d="M492 109.5c-17.4 7.7-36 12.9-55.6 15.3 20-12 35.4-31 42.6-53.6 -18.7 11.1-39.4 19.2-61.5 23.5C399.8 75.8 374.6 64 346.8 64c-53.5 0-96.8 43.4-96.8 96.9 0 7.6 0.8 15 2.5 22.1 -80.5-4-151.9-42.6-199.6-101.3 -8.3 14.3-13.1 31-13.1 48.7 0 33.6 17.2 63.3 43.2 80.7C67 210.7 52 206.3 39 199c0 0.4 0 0.8 0 1.2 0 47 33.4 86.1 77.7 95 -8.1 2.2-16.7 3.4-25.5 3.4 -6.2 0-12.3-0.6-18.2-1.8 12.3 38.5 48.1 66.5 90.5 67.3 -33.1 26-74.9 41.5-120.3 41.5 -7.8 0-15.5-0.5-23.1-1.4C62.8 432 113.7 448 168.3 448 346.6 448 444 300.3 444 172.2c0-4.2-0.1-8.4-0.3-12.5C462.6 146 479 129 492 109.5z"></path></svg>
            </a>
            <a href="https://www.facebook.com/sharer/sharer.php?u=https://gearons.org//posts/tlstm" title="Share on Facebook" rel="noreferrer noopener" target="_blank">
              <svg viewbox="0 0 512 512"><path d="M288 192v-38.1c0-17.2 3.8-25.9 30.5-25.9H352V64h-55.9c-68.5 0-91.1 31.4-91.1 85.3V192h-45v64h45v192h83V256h56.4l7.6-64H288z"></path></svg>
            </a>
          </div>

          
        </article>
        <footer class="footer scrollappear">
  <p>
    Opinions are my own. Hoang NT, 2020.
  </p>
</footer>

      </div>
    </div>
  </main>
  

<script type="text/javascript" src="/assets/vendor-2c224c53eb697c739f9490c38819a72184f09472739fd9e492272ef174090428.js"></script>


  <script type="text/javascript" src="/assets/webfonts-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.js"></script>



  <script type="text/javascript" src="/assets/scrollappear-e2da8ea567e418637e31266cc5302126eaa79f62a2273739086358b589a89ee6.js"></script>


<script type="text/javascript" src="/assets/application-cfde13ac81ddaf4351b2e739603e2baf688d0fcc9aba613fe62bbb1c7b037fb9.js"></script>



</body>
</html>
