<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Hoang NT


  | Mini project - Predicting Boston housing price

</title>
<meta name="description" content="Hoang NT's personal website.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="/assets/css/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚙️</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2016/boston-housing/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N8K9D9WKRW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-N8K9D9WKRW');
</script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://gearons.org/">
       Hoang NT
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Mini project - Predicting Boston housing price</h1>
    <p class="post-meta">December 15, 2016</p>
  </header>

  <article class="post-content">
    <h2 id="dataset-boston-housing">Dataset: Boston housing</h2>

<p><em>First project of Udacity Machine Learning Nanodegree</em></p>

<p>A description of the dataset can be found <a href="https://archive.ics.uci.edu/ml/datasets/Housing">here</a>. This dataset concerns housing values in suburbs of Boston. There is <strong>506 instances</strong> of <strong>14 attributes</strong> each in the dataset. Generally, this dataset is suitable for regression task. Attributes in the datasets:</p>

<ul>
  <li>CRIM: Per capita crime rate by town.</li>
  <li>ZN: Proportion of residental land zoned for lots over 25,000 sq.ft.</li>
  <li>INDUS: Proportion of non-retail business acres per town.</li>
  <li>CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).</li>
  <li>NOX: Nitric oxides concentration (parts per 10 million).</li>
  <li>RM: Average number of rooms per dwelling.</li>
  <li>AGE: Proportion of owner-occupied units built prior to 1940.</li>
  <li>DIS: Weighted distances to five Boston employment centres.</li>
  <li>RAD: Index of accessibility to radial highways.</li>
  <li>TAX: Full-value property-tax rate per $10,000.</li>
  <li>PTRATIO: pupil-teacher ratio by town.</li>
  <li>B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.</li>
  <li>LSTAT: % lower status of the population.</li>
  <li>MEDV: Median value of owner-occupied homes in $1000’s (usually the target).</li>
</ul>

<p>The Boston housing dataset is shipped with <code class="language-plaintext highlighter-rouge">scikit-learn</code>. The same description of the data as above can be obtained from <code class="language-plaintext highlighter-rouge">scikit-learn.datasets.load_boston().DESCR</code>.</p>

<h2 id="requirement">Requirement</h2>

<p>This project requires <code class="language-plaintext highlighter-rouge">Python-3.5.2</code>, <code class="language-plaintext highlighter-rouge">jupyter-1.0.0</code>, <code class="language-plaintext highlighter-rouge">numpy-1.11.2</code>, <code class="language-plaintext highlighter-rouge">scikit-learn-0.18.1</code>, and <code class="language-plaintext highlighter-rouge">matplotlib-1.5.3</code> installed. I recommend to use Anaconda to manage Python virtual environments and packages.</p>

<p>First, we import necessary packages:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">numpy</code> for numeric computations.</li>
  <li><code class="language-plaintext highlighter-rouge">matplotlib.pyplot</code> for visualization. (inline means the figures are shown in the notebook)</li>
  <li><code class="language-plaintext highlighter-rouge">sklearn</code> for boston housing dataset and decision tree model.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td class="code"><pre><span class="c1"># Importing a few necessary libraries
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">pl</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="c1"># Make matplotlib show plots inline
</span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># Create our client's feature set for 
# which we will be predicting a selling price
</span><span class="n">CLIENT_FEATURES</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">11.95</span><span class="p">,</span> <span class="mf">0.00</span><span class="p">,</span> <span class="mf">18.100</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.6590</span><span class="p">,</span> <span class="mf">5.6090</span><span class="p">,</span> <span class="mf">90.00</span><span class="p">,</span> \
                    <span class="mf">1.385</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mf">680.0</span><span class="p">,</span> <span class="mf">20.20</span><span class="p">,</span> <span class="mf">332.09</span><span class="p">,</span> <span class="mf">12.13</span><span class="p">]]</span>

<span class="c1"># Load the Boston Housing dataset into the city_data variable
</span><span class="n">city_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">load_boston</span><span class="p">()</span>

<span class="c1"># Initialize the housing prices and housing features
</span><span class="n">housing_prices</span> <span class="o">=</span> <span class="n">city_data</span><span class="p">.</span><span class="n">target</span>
<span class="n">housing_features</span> <span class="o">=</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Boston Housing dataset loaded successfully!"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Boston Housing dataset loaded successfully!
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="n">city_data</span><span class="p">.</span><span class="n">feature_names</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',
       'TAX', 'PTRATIO', 'B', 'LSTAT'], 
      dtype='&lt;U7')
</code></pre></div></div>

<p>I would like to see the data in a nice table format, so I load the data into a <code class="language-plaintext highlighter-rouge">pandas.DataFrame</code> and printed the first five rows with <code class="language-plaintext highlighter-rouge">.head()</code>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre><span class="n">pdict</span> <span class="o">=</span> <span class="p">{</span><span class="s">'CRIM'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> 
         <span class="s">'ZN'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> 
         <span class="s">'INDUS'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> 
         <span class="s">'CHAS'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">3</span><span class="p">],</span> 
         <span class="s">'NOX'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">4</span><span class="p">],</span> 
         <span class="s">'RM'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">5</span><span class="p">],</span>
         <span class="s">'AGE'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">6</span><span class="p">],</span> 
         <span class="s">'DIS'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">7</span><span class="p">],</span> 
         <span class="s">'RAD'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">8</span><span class="p">],</span> 
         <span class="s">'TAX'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">9</span><span class="p">],</span> 
         <span class="s">'PTRATIO'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">10</span><span class="p">],</span> 
         <span class="s">'B'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">11</span><span class="p">],</span> 
         <span class="s">'LSTAT'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span><span class="mi">12</span><span class="p">],</span> 
         <span class="s">'MEDV'</span><span class="p">:</span> <span class="n">city_data</span><span class="p">.</span><span class="n">target</span><span class="p">[:]}</span>
<span class="n">ptable</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pdict</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="n">ptable</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AGE</th>
      <th>B</th>
      <th>CHAS</th>
      <th>CRIM</th>
      <th>DIS</th>
      <th>INDUS</th>
      <th>LSTAT</th>
      <th>MEDV</th>
      <th>NOX</th>
      <th>PTRATIO</th>
      <th>RAD</th>
      <th>RM</th>
      <th>TAX</th>
      <th>ZN</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>65.2</td>
      <td>396.90</td>
      <td>0.0</td>
      <td>0.00632</td>
      <td>4.0900</td>
      <td>2.31</td>
      <td>4.98</td>
      <td>24.0</td>
      <td>0.538</td>
      <td>15.3</td>
      <td>1.0</td>
      <td>6.575</td>
      <td>296.0</td>
      <td>18.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>78.9</td>
      <td>396.90</td>
      <td>0.0</td>
      <td>0.02731</td>
      <td>4.9671</td>
      <td>7.07</td>
      <td>9.14</td>
      <td>21.6</td>
      <td>0.469</td>
      <td>17.8</td>
      <td>2.0</td>
      <td>6.421</td>
      <td>242.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>61.1</td>
      <td>392.83</td>
      <td>0.0</td>
      <td>0.02729</td>
      <td>4.9671</td>
      <td>7.07</td>
      <td>4.03</td>
      <td>34.7</td>
      <td>0.469</td>
      <td>17.8</td>
      <td>2.0</td>
      <td>7.185</td>
      <td>242.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>45.8</td>
      <td>394.63</td>
      <td>0.0</td>
      <td>0.03237</td>
      <td>6.0622</td>
      <td>2.18</td>
      <td>2.94</td>
      <td>33.4</td>
      <td>0.458</td>
      <td>18.7</td>
      <td>3.0</td>
      <td>6.998</td>
      <td>222.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>54.2</td>
      <td>396.90</td>
      <td>0.0</td>
      <td>0.06905</td>
      <td>6.0622</td>
      <td>2.18</td>
      <td>5.33</td>
      <td>36.2</td>
      <td>0.458</td>
      <td>18.7</td>
      <td>3.0</td>
      <td>7.147</td>
      <td>222.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="statistical-analysis-and-data-exploration">Statistical Analysis and Data Exploration</h2>

<p>Let’s quickly investigate a few basic statistics about the dataset and look at the <code class="language-plaintext highlighter-rouge">CLIENT_FEATURES</code> to see how the data relates to it.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
</pre></td><td class="code"><pre><span class="c1"># Number of houses and features in the dataset
</span><span class="n">total_houses</span><span class="p">,</span> <span class="n">total_features</span> <span class="o">=</span> <span class="n">city_data</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span>

<span class="c1"># Minimum housing value in the dataset
</span><span class="n">minimum_price</span> <span class="o">=</span> <span class="n">housing_prices</span><span class="p">.</span><span class="nb">min</span><span class="p">()</span>

<span class="c1"># Maximum housing value in the dataset
</span><span class="n">maximum_price</span> <span class="o">=</span> <span class="n">housing_prices</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span>

<span class="c1"># Mean house value of the dataset
</span><span class="n">mean_price</span> <span class="o">=</span> <span class="n">housing_prices</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Median house value of the dataset
</span><span class="n">median_price</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">median</span><span class="p">(</span><span class="n">housing_prices</span><span class="p">)</span>

<span class="c1"># Standard deviation of housing values of the dataset
</span><span class="n">std_dev</span> <span class="o">=</span> <span class="n">housing_prices</span><span class="p">.</span><span class="n">std</span><span class="p">()</span>

<span class="c1"># Show the calculated statistics
</span><span class="k">print</span><span class="p">(</span><span class="s">"Boston Housing dataset statistics (in $1000's):</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Total number of houses:"</span><span class="p">,</span> <span class="n">total_houses</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Total number of features:"</span><span class="p">,</span> <span class="n">total_features</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Minimum house price:"</span><span class="p">,</span> <span class="n">minimum_price</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Maximum house price:"</span><span class="p">,</span> <span class="n">maximum_price</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Mean house price: {0:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">mean_price</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Median house price:"</span><span class="p">,</span> <span class="n">median_price</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Standard deviation of house price: {0:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">std_dev</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Boston Housing dataset statistics (in $1000's):

Total number of houses: 506
Total number of features: 13
Minimum house price: 5.0
Maximum house price: 50.0
Mean house price: 22.533
Median house price: 21.2
Standard deviation of house price: 9.188
</code></pre></div></div>

<p>By intuition, the top 3 deciding factors is crime rate (CRIM), proportion of blacks (B), and the accessibility to the highway (RAD).</p>

<ul>
  <li><strong>CRIM</strong>: Area with low crime rate must have higher security, income, insurrance, and better life in general. Hence the price of houses must be affected by this factor.</li>
  <li><strong>B</strong>: Many people might think that area with many blacks will have be not so safe. Therefore the price might be higher for residence with smaller blacks porpotion.</li>
  <li><strong>RAD</strong>: The accessibility to the highway might also be desirable as it is more convenient to go to work.</li>
</ul>

<p>Let’s examine our client. There features we selected have the index <code class="language-plaintext highlighter-rouge">0</code> (CRIM), <code class="language-plaintext highlighter-rouge">8</code> (RAD), and <code class="language-plaintext highlighter-rouge">11</code> (B).</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="k">print</span><span class="p">(</span><span class="n">CLIENT_FEATURES</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[11.95, 0.0, 18.1, 0, 0.659, 5.609, 90.0, 1.385, 24, 680.0, 20.2, 332.09, 12.13]]
</code></pre></div></div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="code"><pre><span class="k">print</span><span class="p">(</span><span class="s">'Client CRIM = '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">CLIENT_FEATURES</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Client RAD = '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">CLIENT_FEATURES</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">8</span><span class="p">]))</span> 
<span class="k">print</span><span class="p">(</span><span class="s">'Client B = '</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">CLIENT_FEATURES</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">11</span><span class="p">]))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Client CRIM = 11.95
Client RAD = 24
Client B = 332.09
</code></pre></div></div>

<p>Our client’s crime rate is quite high!</p>

<h2 id="picking-evaluation-method">Picking evaluation method</h2>

<p>We first shuffle the data using <code class="language-plaintext highlighter-rouge">sklearn.utils.shuffle(*arrays, *options)</code>. This function will return new shuffled data and target arrays. Then we split data 70-30 to use for training and testing using <code class="language-plaintext highlighter-rouge">train_test_split(...)</code>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="code"><pre><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">shuffle</span>

<span class="k">def</span> <span class="nf">shuffle_split_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="s">""" 
    Shuffles and splits data into 70% training and 30% testing subsets,
    then returns the training and testing subsets. 
    """</span>
    <span class="c1"># Shuffled data
</span>    <span class="n">X_s</span><span class="p">,</span> <span class="n">y_s</span> <span class="o">=</span> <span class="n">shuffle</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Split the data into training (70%) and testing (30%)
</span>    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_s</span><span class="p">,</span> <span class="n">y_s</span><span class="p">,</span>
                                                        <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                                                        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Return the training and testing data subsets
</span>    <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span>


<span class="c1"># Test shuffle_split_data
</span><span class="k">try</span><span class="p">:</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">shuffle_split_data</span><span class="p">(</span><span class="n">housing_features</span><span class="p">,</span> 
                                                          <span class="n">housing_prices</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Successfully shuffled and split the data!"</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Something went wrong with shuffling and splitting the data."</span><span class="p">)</span>
    
<span class="k">print</span><span class="p">(</span><span class="s">"Shape of training data: "</span><span class="p">,</span> <span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Shape of training target: "</span><span class="p">,</span> <span class="n">y_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Shape of testing data: "</span><span class="p">,</span> <span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Shape of testing target: "</span><span class="p">,</span> <span class="n">y_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Successfully shuffled and split the data!
Shape of training data:  (354, 13)
Shape of training target:  (354,)
Shape of testing data:  (152, 13)
Shape of testing target:  (152,)
</code></pre></div></div>

<p>Splitting the data for training and testing allows us to evaluate our model by looking at the performance on training and testing data. The learning curves for training and testing show us if the model is underfitting (bias) or overfitting (variation).</p>

<p>MSE or MAE are better choices for regression task. Metrics like accuracy, precision, recall, f1-score are often used for evaluating a classification problem.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="code"><pre><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span> <span class="k">as</span> <span class="n">MAE</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span> <span class="k">as</span> <span class="n">MSE</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="k">def</span> <span class="nf">performance_metric</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_predict</span><span class="p">):</span>
    <span class="s">""" 
    Calculates and returns the total error between true 
    and predicted values
    based on a performance metric chosen by the student. 
    """</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">MAE</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_predict</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">error</span>

<span class="c1"># Test performance_metric
</span><span class="k">try</span><span class="p">:</span>
    <span class="n">total_error</span> <span class="o">=</span> <span class="n">performance_metric</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Successfully performed a metric calculation!"</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Something went wrong with performing a metric calculation."</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Successfully performed a metric calculation!
</code></pre></div></div>

<p>As mentioned before, mean squared error (MSE) and mean absolute error (MAE) are both appropriate for predicting housing prices. MAE is robust to outlier but it is not always differentible for grtadient methods. In contrast, MSE is always differentible but it weights the high loss (outlier) heavily. Since Boston dataset contains many outliers, where housing price is high for some special reason, MAE is the most appropriate error evaluation.</p>

<p><code class="language-plaintext highlighter-rouge">fit_model</code> performs grid search cross validation and return the best estimator. <a href="http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html">GridSearchCV</a> is the object provided by <code class="language-plaintext highlighter-rouge">scikit-learn</code> to search for the best paratemeters using cross-validation and then return the best estimator. To use <code class="language-plaintext highlighter-rouge">GridSearchCV</code>, we need to pass the <code class="language-plaintext highlighter-rouge">estimator</code>, the dictionary containing the parameter grid <code class="language-plaintext highlighter-rouge">param_grid</code>, the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html">scrorer callable</a> object <code class="language-plaintext highlighter-rouge">scoring</code>, and optionally the number of cross-validation fold <code class="language-plaintext highlighter-rouge">cv</code>. Note that when we use error metrics (MAE, MSE, etc.), we need to specify <code class="language-plaintext highlighter-rouge">greater_is_better = False</code> in <code class="language-plaintext highlighter-rouge">make_scorer</code> function.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
</pre></td><td class="code"><pre><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">make_scorer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="k">def</span> <span class="nf">fit_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="s">"""
    Tunes a decision tree regressor 
    model using GridSearchCV on the input data X 
    and target labels y and returns this optimal model.
    """</span>

    <span class="c1"># Create a decision tree regressor object
</span>    <span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">()</span>

    <span class="c1"># Set up the parameters we wish to tune
</span>    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">'max_depth'</span><span class="p">:(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">)}</span>

    <span class="c1"># Make an appropriate scoring function
</span>    <span class="n">scoring_function</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">performance_metric</span><span class="p">,</span> 
                                   <span class="n">greater_is_better</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="c1"># Make the GridSearchCV object
</span>    <span class="n">reg</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">regressor</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> 
                       <span class="n">scoring_function</span><span class="p">)</span>

    <span class="c1"># Fit the learner to the data to obtain the optimal 
</span>    <span class="c1"># model with tuned parameters
</span>    <span class="n">reg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Return the optimal model
</span>    <span class="k">return</span> <span class="n">reg</span><span class="p">.</span><span class="n">best_estimator_</span>

<span class="c1"># Test fit_model on entire dataset
</span><span class="k">try</span><span class="p">:</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="n">fit_model</span><span class="p">(</span><span class="n">housing_features</span><span class="p">,</span> <span class="n">housing_prices</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Successfully fit a model!"</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Something went wrong with fitting a model."</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Successfully fit a model!
</code></pre></div></div>

<p>Grid search algorithm is a brute-force hyper-parameter search for the best estimator configuration. Grid search algorithm is applicable when we need to find the best parameters for our learning model. This algorithm searches through all possible hyper-parameter configurations, evaluates the error of each configuration, then returns the best one. The exhaustive search guarantee the best model configuration is returned. However, due to the nature of a brute force algorithm, grid search might not be suitable for models with a large number of hyper-parameters or the hyper-parameters have large search spaces.</p>

<p>Cross-validation is a data reuse technique to maximize the usage of data for training and testing. Specifying an integer <code class="language-plaintext highlighter-rouge">k</code> in advanced, for each model, cross-validation scheme splits the given training data into k-fold, runs the training procedure k times with k-1 folds of data as training and the remaining 1 fold as testing data. The final error is then averaged for k folds. Based on this cross-validation error, we can evaluate our model for overfitting. In contrast, if we evaluate the error of our model on the training dataset, there is a high chance that the learning algorithm will overfit the data (it can just remember the exact input-output without generalizing the data). In grid search, each model’s configuration might have different performance on the training dataset. Without cross-validation, the grid search algorithm might select the model configuration that best <em>overfits</em> the data. On the other hand, with cross-validation, grid search can account for variation in the model’s prediction and prevent overfitting.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">learning_curves</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="s">"""
    Calculates the performance of several models with 
    varying sizes of training data. The learning and testing 
    error rates for each model are then plotted. 
    """</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"Creating learning curve graphs for max_depths of 1, 3, 6, and 10. . ."</span><span class="p">)</span>
    
    <span class="c1"># Create the figure window
</span>    <span class="n">fig</span> <span class="o">=</span> <span class="n">pl</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

    <span class="c1"># We will vary the training set size so that 
</span>    <span class="c1"># we have 50 different sizes
</span>    <span class="n">sizes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">rint</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="mi">50</span><span class="p">)).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">train_err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">))</span>
    <span class="n">test_err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">))</span>

    <span class="c1"># Create four different models based on max_depth
</span>    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">depth</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">]):</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sizes</span><span class="p">):</span>
            
            <span class="c1"># Setup a decision tree regressor so that 
</span>            <span class="c1"># it learns a tree with max_depth = depth
</span>            <span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">depth</span><span class="p">)</span>
            
            <span class="c1"># Fit the learner to the training data
</span>            <span class="n">regressor</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="n">s</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="n">s</span><span class="p">])</span>

            <span class="c1"># Find the performance on the training set
</span>            <span class="n">train_err</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">performance_metric</span><span class="p">(</span><span class="n">y_train</span><span class="p">[:</span><span class="n">s</span><span class="p">],</span> 
                                              <span class="n">regressor</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="n">s</span><span class="p">]))</span>
            
            <span class="c1"># Find the performance on the testing set
</span>            <span class="n">test_err</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">performance_metric</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> 
                                             <span class="n">regressor</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

        <span class="c1"># Subplot the learning curve graph
</span>        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">test_err</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'Testing Error'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">train_err</span><span class="p">,</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'Training Error'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'max_depth = %s'</span><span class="o">%</span><span class="p">(</span><span class="n">depth</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Number of Data Points in Training Set'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Total Error'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)])</span>
    
    <span class="c1"># Visual aesthetics
</span>    <span class="n">fig</span><span class="p">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s">'Decision Tree Regressor Learning Performances'</span><span class="p">,</span> 
                 <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.03</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">model_complexity</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="s">""" 
    Calculates the performance of the model 
    as model complexity increases. The learning and 
    testing errors rates are then plotted. 
    """</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"Creating a model complexity graph. . . "</span><span class="p">)</span>

    <span class="c1"># We will vary the max_depth of a decision tree 
</span>    <span class="c1"># model from 1 to 14
</span>    <span class="n">max_depth</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">14</span><span class="p">)</span>
    <span class="n">train_err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">max_depth</span><span class="p">))</span>
    <span class="n">test_err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">max_depth</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">max_depth</span><span class="p">):</span>
        <span class="c1"># Setup a Decision Tree Regressor so that it learns 
</span>        <span class="c1"># a tree with depth d
</span>        <span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">d</span><span class="p">)</span>

        <span class="c1"># Fit the learner to the training data
</span>        <span class="n">regressor</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="c1"># Find the performance on the training set
</span>        <span class="n">train_err</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">performance_metric</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> 
                                          <span class="n">regressor</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>

        <span class="c1"># Find the performance on the testing set
</span>        <span class="n">test_err</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">performance_metric</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> 
                                         <span class="n">regressor</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>

    <span class="c1"># Plot the model complexity graph
</span>    <span class="n">pl</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Decision Tree Regressor Complexity Performance'</span><span class="p">)</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">test_err</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'Testing Error'</span><span class="p">)</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">train_err</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">'Training Error'</span><span class="p">)</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Maximum Depth'</span><span class="p">)</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Total Error'</span><span class="p">)</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></figure>

<h2 id="analyzing-model-performance">Analyzing Model Performance</h2>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="n">learning_curves</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Creating learning curve graphs for max_depths of 1, 3, 6, and 10. . .
</code></pre></div></div>

<p><img src="/img/boston_housing_29_1.png" alt="png" /></p>

<p>The <code class="language-plaintext highlighter-rouge">learning_curves</code> function trains the input data with 4 different max-depth values and then plot the learning curves as above. The <code class="language-plaintext highlighter-rouge">DecisionTreeRegressor</code> models are trained on the training dataset with increasing data size (50 models in total). The error of prediction is then measured on the training data used (green line) and the testing data (blue line).</p>

<p><code class="language-plaintext highlighter-rouge">max_depth = 6</code> curve shows large variation as we increase the training data size. Throughout the training processes, the training error was small and it increased a little as the training size increased. At the same time, we can see the testing error had an overall downward trend, but no clear sign of convergence. Besides, the testing error has a large variation while the testing error is small hints that our model might overfit the training data.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
</pre></td><td class="code"><pre><span class="n">model_complexity</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Creating a model complexity graph. . . 
</code></pre></div></div>

<p><img src="/img/boston_housing_32_1.png" alt="png" /></p>

<p>The figure above shows the model compexity curve. The function <code class="language-plaintext highlighter-rouge">model_complexity</code> trains each learning models using the whole training data and then computes the error rate on training and testing error. When the <code class="language-plaintext highlighter-rouge">max_depth</code> is 1, both testing and training error is high. In this case, the model could not generalize the data well, lead to high bias. On the other hand, when <code class="language-plaintext highlighter-rouge">max_depth</code> is 10, we have a small training error, but large testing error. The model might overfitted the training data, hence high variance.</p>

<p>Based on the model complexity curve, we can say the best <code class="language-plaintext highlighter-rouge">max_depth</code> value is probably within the range 4 to 8. Any value lower than 4 leads to high bias, while values larger than 8 lead to high variance.</p>

<h2 id="model-prediction">Model Prediction</h2>

<p>In the previous code block, we have defined the function <code class="language-plaintext highlighter-rouge">fit_model</code> in which the <code class="language-plaintext highlighter-rouge">max_depth</code> is selected by <code class="language-plaintext highlighter-rouge">GridSearchCV</code>. As expected from observing the model compexity curve above, this function yields <code class="language-plaintext highlighter-rouge">max_depth</code> of 5 in MAE and MSE error metrics.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre><span class="n">max_depths</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="n">fit_model</span><span class="p">(</span><span class="n">housing_features</span><span class="p">,</span> <span class="n">housing_prices</span><span class="p">)</span>
    <span class="n">max_depths</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">reg</span><span class="p">.</span><span class="n">get_params</span><span class="p">()[</span><span class="s">'max_depth'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"GridSearchCV max_depth result for DecisionTreeRegression model: "</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Median:"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">median</span><span class="p">(</span><span class="n">max_depths</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Mean:"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">max_depths</span><span class="p">),</span> <span class="s">", Standard deviation:"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">max_depths</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GridSearchCV max_depth result for DecisionTreeRegression model: 
Median: 5.0
Mean: 5.06 , Standard deviation: 1.1297787394
</code></pre></div></div>

<p>The code block below gives the prediction for our client:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre><span class="n">sale_price</span> <span class="o">=</span> <span class="n">reg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">CLIENT_FEATURES</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Predicted value of client's home: {0:.3f}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">sale_price</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></td></tr></tbody></table></code></pre></figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Predicted value of client's home: 20.766
</code></pre></div></div>

<p>Compared to the basic statistic, our client’s home value is well below the mean and the median, but it is within the standard deriviation. Based on the learning and testing curves above, we can see that this model is about $3,000 off in prediction on average. Compared to the $20,000 price range, model is quite accurate. However, in order to put this model into real use, the human prediction error should be compared with this model. If this model perform better or similar to human error, I will definitely use this model as the tool for clients. Otherwise, this model can only serve as an reference point estimation for human evaluation. Personally I doubt human prediction and believes in statistics more. As mentioned in the book “Thinking fast and slow” by Daniel Kahneman, human judgements are biased.</p>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2022 Hoang  NT.
    
    
    
    Last updated: October 11, 2022.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
